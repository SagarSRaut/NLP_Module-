{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8b29bbe-7fae-4bcd-a394-1e87dcc9c9e0",
   "metadata": {},
   "source": [
    "### Library installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e161c1a1-ffa5-4dba-aab5-8e8413d514d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (3.7.4)\n",
      "Requirement already satisfied: textblob in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: click in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk spacy textblob -U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19b7f11e-fc64-4da3-8225-cf990769c058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "698506ec-eb49-4c2b-93a3-76993e925002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package indian to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package indian is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') # tokenization\n",
    "nltk.download('stopwords') # stopwords removal\n",
    "nltk.download('averaged_perceptron_tagger') # POS (part of speech) tagging\n",
    "nltk.download('wordnet') # wordnet database and lemmatization\n",
    "nltk.download('omw-1.4')  #stemming\n",
    "nltk.download('indian') # Indian language POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac1bc3f0-98c8-4e3f-acbb-5a75f7ccaea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker') #chunker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37527059-42fc-4a64-aa9b-d48a00e789c0",
   "metadata": {},
   "source": [
    "#### Sample Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c9f49e9-0386-41b7-96f3-3e0d029e2c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'They told that their ages are 25 27 and 31 respectuvely.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17b56f38-acfb-4fed-a9c2-829d2b1d5ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the average of ages mentioned in the above mentioned sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4971162e-0372-495a-a633-4ac3a637867a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.666666666666668\n"
     ]
    }
   ],
   "source": [
    "# for i in range(len(sent)):\n",
    "#     if sent.isdigit():\n",
    "#         print(sent.find[i])\n",
    "\n",
    "lst = []\n",
    "lst1 = []\n",
    "\n",
    "lst = sent.split(' ')\n",
    "for i in lst:\n",
    "    if i.isnumeric():\n",
    "        lst1.append(i)\n",
    "print(sum([int(i) for i in lst1])/len(lst1))\n",
    "# print(sum(int(lst1))/len(lst1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9384c1e9-095d-4317-a461-e041e6603251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean([int(word) for word in sent.split() if word.isdigit()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73eecc6a-dc8a-436b-899f-d310a4492f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ages=[]\n",
    "ages=[int(word) for word in sent.split() if word.isdigit()]\n",
    "sum(ages)/len(ages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce5f5dd-6fc9-4149-a88d-ce6b9f39eb6f",
   "metadata": {},
   "source": [
    "### Tokeniztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7e5b0b9-df86-48b0-adb5-e4c9797acf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Hello friends! How are you? Welcome to Python Programming.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbe310f0-9f8e-42a9-9e9f-4941d854ebf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the function \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76979ae9-724c-4c0a-84b8-311124a160a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!', 'How are you?', 'Welcome to Python Programming.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# segmentation\n",
    "sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d356d666-fb0c-4d62-a21f-edcb810550b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f8b6225-942c-4fd3-9956-c0745bcb62ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the percentage of punctuation symbbols present in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1a774bc-4624-4fff-8747-0c0d34b74a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = []\n",
    "# l1 = []\n",
    "# for i in p_punc:\n",
    "#     if i.isalnum():\n",
    "#         l.append(i)\n",
    "#     else:\n",
    "#         l1.append(i)\n",
    "\n",
    "# print(len([int(i) for i in l1])/(sum([int(i) for i in l])+sum([int(i) for i in l1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "176ad7ba-026a-4830-b2fe-562174be4cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punct_count = len([word for word in word_tokenize(sent) if not word.isalnum()])\n",
    "punct_count / len(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c90d657a-6556-48ca-ba86-f333eb789b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " ord('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11fa7ab1-a94b-43ac-afad-4d1775183072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof('=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59ebd57d-0125-4a8a-ab57-78969d56148c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function getsizeof in module sys:\n",
      "\n",
      "getsizeof(...)\n",
      "    getsizeof(object [, default]) -> int\n",
      "    \n",
      "    Return the size of object in bytes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sys.getsizeof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f42785d0-1676-4c57-b6d4-cb51b3059d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char = 'abcdef'\n",
    "sys.getsizeof(char) #50 + 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62bc9d0c-2d33-4578-b4a7-65d3c722eb23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤©'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(2345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7a0fcf0-f13e-4f21-8a6b-0d5cf126a964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤µà¥€'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char = '\\u0935\\u0940'  #unicode\n",
    "char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0d1104e-02e6-4e6b-93af-e4a4b1b90a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤µ'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0x935)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7abd2507-5c9a-489c-b706-9d7c5b7086c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof('à¤µà¥€')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "848f12a9-eb9c-4191-8afe-35333363143e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof('ðŸ˜‚')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9464afa-1426-4490-a943-5d6112271fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'à¤…à¤®à¥‡à¤¯ à¤ªà¤£à¤¶à¥€à¤•à¤°'\n",
    "sys.getsizeof(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c36ad916-189e-4cc0-9afe-f29ee3ead314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.startswith('à¤…')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee3ba391-a219-4df2-a511-d468ba68c092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name[-2].endswith('à¤•')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "116de66e-b865-4f84-b5b5-8b13e8b3665a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.find('à¤£')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5db3ec18-0a36-414d-9486-4e26b6516c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0587314-573b-4fa8-934f-6364510a1838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['à¤°à¥‹à¤¹à¤¿à¤¤', 'à¤°à¥‹à¤µà¤®à¤¨']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = ['à¤µà¤¿à¤°à¤¾à¤Ÿ', 'à¤°à¥‹à¤¹à¤¿à¤¤' ,'à¤šà¤¹à¤²', 'à¤¬à¥à¤®à¤°à¤¾à¤¹ ','à¤°à¥‹à¤µà¤®à¤¨']\n",
    "[i for i in name if i.startswith('à¤°à¥‹')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "351ab3b7-75f3-41e7-b5ee-02ae01054594",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtext = 'à¥ªà¥® à¤•à¤¿. à¤®à¥€. à¤…à¤‚à¤¤à¤°à¤¾à¤µà¤° à¤†à¤£à¤¿ à¤ªà¥à¤£à¥‡ à¤œà¤¿à¤²à¥à¤¹à¥à¤¯à¤¾à¤¤à¥€à¤² à¤µà¥‡à¤²à¥à¤¹à¥‡ à¤¤à¤¾à¤²à¥à¤•à¥à¤¯à¤¾à¤¤ à¤µ à¤­à¥‹à¤° à¤—à¤¾à¤µà¤¾à¤šà¥à¤¯à¤¾ à¤µà¤¾à¤¯à¤µà¥à¤¯à¥‡à¤²à¤¾ à¥¨à¥ª à¤•à¤¿.à¤®à¥€. à¤…à¤‚à¤¤à¤°à¤¾à¤µà¤° à¤¨à¥€à¤°à¤¾-à¤µà¥‡à¤³à¤µà¤‚à¤¡à¥€-à¤•à¤¾à¤¨à¤‚à¤¦à¥€ à¤†à¤£à¤¿ à¤—à¥à¤‚à¤œà¤µà¤£à¥€ à¤¯à¤¾ à¤¨à¤¦à¥à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾ à¤–à¥‹à¤±à¥à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾ à¤¬à¥‡à¤šà¤•à¥à¤¯à¤¾à¤¤ à¤®à¥à¤°à¥à¤‚à¤¬à¤¦à¥‡à¤µà¤¾à¤šà¤¾ à¤¡à¥‹à¤‚à¤—à¤° à¤‰à¤­à¤¾ à¤†à¤¹à¥‡. à¤®à¤¾à¤µà¤³ à¤­à¤¾à¤—à¤¾à¤®à¤§à¥à¤¯à¥‡ à¤°à¤¾à¤œà¥à¤¯à¤µà¤¿à¤¸à¥à¤¤à¤¾à¤° à¤¸à¤¾à¤§à¥à¤¯ à¤•à¤°à¤£à¥à¤¯à¤¾à¤¸à¤¾à¤ à¥€ à¤°à¤¾à¤œà¤—à¤¡ à¤†à¤£à¤¿ à¤¤à¥‹à¤°à¤£à¤¾ à¤¹à¥‡ à¤¦à¥‹à¤¨à¥à¤¹à¥€ à¤•à¤¿à¤²à¥à¤²à¥‡ à¤®à¥‹à¤•à¥à¤¯à¤¾à¤šà¥à¤¯à¤¾ à¤ à¤¿à¤•à¤¾à¤£à¥€ à¤¹à¥‹à¤¤à¥‡. à¤¤à¥‹à¤°à¤£à¤¾ Archived 2020-09-20 at the Wayback Machine. à¤•à¤¿à¤²à¥à¤²à¥à¤¯à¤¾à¤šà¤¾ à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤†à¤•à¤¾à¤°à¤¾à¤¨à¥‡ à¤²à¤¹à¤¾à¤¨ à¤…à¤¸à¤²à¥à¤¯à¤¾à¤®à¥à¤³à¥‡ à¤°à¤¾à¤œà¤•à¥€à¤¯ à¤•à¥‡à¤‚à¤¦à¥à¤° à¤®à¥à¤¹à¤£à¥‚à¤¨ à¤¹à¤¾ à¤•à¤¿à¤²à¥à¤²à¤¾ à¤¸à¥‹à¤¯à¥€à¤šà¤¾ à¤¨à¤µà¥à¤¹à¤¤à¤¾. à¤¤à¥à¤¯à¤¾à¤®à¤¾à¤¨à¤¾à¤¨à¥‡ à¤°à¤¾à¤œà¤—à¤¡ à¤¦à¥à¤°à¥à¤—à¤® à¤…à¤¸à¥‚à¤¨ à¤¤à¥à¤¯à¤¾à¤šà¤¾ à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤¬à¤°à¤¾à¤š à¤®à¥‹à¤ à¤¾ à¤†à¤¹à¥‡. à¤¶à¤¿à¤µà¤¾à¤¯ à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤•à¤¡à¥‡ à¤•à¥‹à¤£à¤¤à¥à¤¯à¤¾à¤¹à¥€ à¤¬à¤¾à¤œà¥‚à¤¨à¥‡ à¤¯à¥‡à¤¤à¤¾à¤¨à¤¾ à¤à¤–à¤¾à¤¦à¥€ à¤Ÿà¥‡à¤•à¤¡à¥€ à¤•à¤¿à¤‚à¤µà¤¾ à¤¨à¤¦à¥€ à¤“à¤²à¤¾à¤‚à¤¡à¤¾à¤µà¥€à¤š à¤²à¤¾à¤—à¤¤à¥‡. à¤à¤µà¤¢à¥€ à¤¸à¥à¤°à¤•à¥à¤·à¤¿à¤¤à¤¤à¤¾ à¤¹à¥‹à¤¤à¥€,à¤®à¥à¤¹à¤£à¥‚à¤¨ à¤†à¤ªà¤²à¥‡ à¤°à¤¾à¤œà¤•à¥€à¤¯ à¤•à¥‡à¤‚à¤¦à¥à¤° à¤®à¥à¤¹à¤£à¥‚à¤¨ à¤¶à¤¿à¤µà¤¾à¤œà¥€ à¤®à¤¹à¤¾à¤°à¤¾à¤œà¤¾à¤‚à¤¨à¥€'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30986bc7-88ac-42dd-a1da-aaec08ea8192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['à¥ªà¥®', 'à¤•à¤¿', '.', 'à¤®à¥€', '.', 'à¤…à¤‚à¤¤à¤°à¤¾à¤µà¤°', 'à¤†à¤£à¤¿', 'à¤ªà¥à¤£à¥‡', 'à¤œà¤¿à¤²à¥à¤¹à¥à¤¯à¤¾à¤¤à¥€à¤²', 'à¤µà¥‡à¤²à¥à¤¹à¥‡']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(mtext)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53ded94-9dea-41d2-bc6e-64b7014f83c4",
   "metadata": {},
   "source": [
    "### Space Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ffa00c42-9e66-4b40-8ede-cde4433c92bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends!\t How are you?\n",
      "Welcome to the world of\t Python Programming.\n"
     ]
    }
   ],
   "source": [
    "f = open('mydata.txt')\n",
    "data = f.read()\n",
    "print(data)\n",
    "\n",
    "#read ==> reads the complete file and stores it as a string\n",
    "#readable ==> returns true or false based on whether the file is readable or not\n",
    "#readline ==> returns a single line as a string\n",
    "#readlines ==> returns multiple lines and returns a list of strings (one element = one line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c06ddf50-0820-4005-b3f8-499da502f7d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends!\\t',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you?\\nWelcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of\\t',\n",
       " 'Python',\n",
       " 'Programming.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import class\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "\n",
    "#create object\n",
    "tk = SpaceTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4706b64e-3803-46ab-a454-c38e9299dc16",
   "metadata": {},
   "source": [
    "  ### Tab tokenizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c4edca9-624e-4805-9bd8-43bf9abc4ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Friends!',\n",
       " ' How are you?\\nWelcome to the world of',\n",
       " ' Python Programming.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import class\n",
    "from nltk.tokenize import TabTokenizer\n",
    "\n",
    "#create object\n",
    "tk = TabTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d741703e-521a-406c-900f-b6b793ea659a",
   "metadata": {},
   "source": [
    "### LineTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "208a65a4-33cb-45f1-abaa-e3ca612f76e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Friends!\\t How are you?',\n",
       " 'Welcome to the world of\\t Python Programming.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import class\n",
    "from nltk.tokenize import LineTokenizer\n",
    "\n",
    "#create object\n",
    "tk = LineTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce555d2-44a4-4301-979e-fe41180ad94a",
   "metadata": {},
   "source": [
    "### White Space Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10f72e86-b981-4658-8afc-f328d84d3976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'Programming.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import class\n",
    "from nltk.tokenize import WhitespaceTokenizer  #exactly similar to the split method\n",
    "\n",
    "#create object\n",
    "tk = WhitespaceTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3943e565-cee7-4e10-9afa-99d82dfc90a0",
   "metadata": {},
   "source": [
    "### MWE (Multi word tokenizer) Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba5abf13-7494-40fb-9a9b-6d36852b55de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = '''The Van Rossum is Python creator, visiting Pune this week. \n",
    "The development community is very eager to meet Van Rossum.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "478f7494-97d1-4cd6-98e1-7367f7a62614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Van Rossum is Python creator, visiting Pune this week. \\nThe development community is very eager to meet Van Rossum.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31a95fa7-58e9-44e4-b784-ff86b3acc55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Van',\n",
       " 'Rossum',\n",
       " 'is',\n",
       " 'Python',\n",
       " 'creator',\n",
       " ',',\n",
       " 'visiting',\n",
       " 'Pune',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'The',\n",
       " 'development',\n",
       " 'community',\n",
       " 'is',\n",
       " 'very',\n",
       " 'eager',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'Van',\n",
       " 'Rossum',\n",
       " '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce7f21-38c0-488a-a832-075fcf9cca6f",
   "metadata": {},
   "source": [
    "### Tweet  Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "00058cb1-7ec9-456e-af44-0fa3a166c2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " ':)',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " ':D',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " ':',\n",
       " 'C',\n",
       " '.']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import class\n",
    "from nltk.tokenize import TweetTokenizer \n",
    "sent2='Hello Friends :)!How are you? Welcome to the world of :D\tPython Programming :C.'\n",
    "#create object\n",
    "tk = TweetTokenizer()\n",
    "\n",
    " \n",
    "# tokenize the data\n",
    "tk.tokenize(sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7517024-75c3-44c9-9021-8f137b52841e",
   "metadata": {},
   "source": [
    "### Emoji Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1fc28062-81c9-47e7-8d9e-8d73df132b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_emoji = 'ðŸ’¯Hello Friends! welcome ðŸ™dogðŸ¶ ðŸ˜‚ðŸ˜‚ sagar raut ha amcha maanus ahe to java cha topper ahe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3f22310d-9555-4cba-962e-388a9f3be34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ðŸ’¯Hello Friends! welcome ðŸ™dogðŸ¶ ðŸ˜‚ðŸ˜‚ sagar raut ha amcha maanus ahe to java cha topper ahe'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "61ab53e8-8a96-4221-b78b-e1074dc49d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ðŸ’¯',\n",
       " 'Hello',\n",
       " 'Friends',\n",
       " '!',\n",
       " 'welcome',\n",
       " 'ðŸ™',\n",
       " 'dog',\n",
       " 'ðŸ¶',\n",
       " 'ðŸ˜‚',\n",
       " 'ðŸ˜‚',\n",
       " 'sagar',\n",
       " 'raut',\n",
       " 'ha',\n",
       " 'amcha',\n",
       " 'maanus',\n",
       " 'ahe',\n",
       " 'to',\n",
       " 'java',\n",
       " 'cha',\n",
       " 'topper',\n",
       " 'ahe']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer \n",
    "tk = TweetTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "tk.tokenize(sent_emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594da126-65e1-4328-80d2-0b612adf1510",
   "metadata": {},
   "source": [
    "### Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eaf83846-d011-45d1-880b-20777629d8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: \n",
      "This\n",
      "is\n",
      "some\n",
      "text\n",
      "with\n",
      "punctuation\n",
      ">\n",
      "Let's\n",
      "tokenize\n",
      "it\n",
      "Is\n",
      "it\n",
      "ok\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return re.split(r\"[.,;?!\\s]+\", text)\n",
    "text = \"This is some text with punctuation > Let's tokenize it. Is it ok?\"\n",
    "\n",
    "tokens = custom_tokenizer(text)\n",
    "print('Tokens: ')\n",
    "\n",
    "for token in tokens:\n",
    "    print(token)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fae197db-84b8-492e-a805-94b64a32e9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('student3.tsv')\n",
    "data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7c5731f1-1ecb-4f96-aa38-3d08a1dd5e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roll\tname\tclass\tmarks\tage\n",
      "1\tanil\tTE\t56.77\t22\n",
      "2\tamit\tTE\t59.77\t21\n",
      "3\taniket\tBE\t76.88\t19\n",
      "4\tajinkya\tTE\t69.66\t20\n",
      "5\tasha\tTE\t63.28\t20\n",
      "6\tayesha\tBE\t49.55\t20\n",
      "7\tamar\tBE\t65.34\t19\n",
      "8\tamita\tBE\t68.33\t23\n",
      "9\tamol\tTE\t56.75\t20\n",
      "10\tanmol\tBE\t78.66\t21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "546f3885-2772-41ae-85d3-75388e99767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = data.split('\\n')[1:]\n",
    "# def custom_tokenizer(out):\n",
    "#     return re.split().replace('\\t',' ')\n",
    "    \n",
    "# tokens = custom_tokenizer(str(out))\n",
    "# print('Tokens: ')\n",
    "\n",
    "# for token in tokens:\n",
    "#     print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "69f2d798-ea0d-4a53-bc7c-f54d675112af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'anil', 'TE', 56.77, 22],\n",
       " [2, 'amit', 'TE', 59.77, 21],\n",
       " [3, 'aniket', 'BE', 76.88, 19],\n",
       " [4, 'ajinkya', 'TE', 69.66, 20],\n",
       " [5, 'asha', 'TE', 63.28, 20],\n",
       " [6, 'ayesha', 'BE', 49.55, 20],\n",
       " [7, 'amar', 'BE', 65.34, 19],\n",
       " [8, 'amita', 'BE', 68.33, 23],\n",
       " [9, 'amol', 'TE', 56.75, 20],\n",
       " [10, 'anmol', 'BE', 78.66, 21],\n",
       " ['']]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdata = []\n",
    "\n",
    "for x in data.split('\\n'):\n",
    "    inner_list  = []\n",
    "    for y in x.split('\\t'):\n",
    "        if y.isdigit():\n",
    "            inner_list.append(int(y))\n",
    "        elif y.find('.') > 0:\n",
    "            inner_list.append(float(y))\n",
    "        else:\n",
    "            inner_list.append(y)\n",
    "    newdata.append(inner_list)\n",
    "newdata[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d8486d-a372-493c-88a8-1e1fb9cdb9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
